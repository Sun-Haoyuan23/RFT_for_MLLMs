<div align="center">

# Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models

</div>

<div align="left">
    "The senses are the organs by which man perceives the world, and the soul acts through them as through tools."
</div>
<div align="right">
â€” Leonardo da Vinci
</div>

![image](RFT_for_MLLMs.jpg)
**Figure 1: An overview of the works done on reinforcement fine-tuning (RFT) for multimodal large language models (MLLMs). Works are sorted by release time and are collected up to May 15, 2025.** 

In this repository, we present a comprehensive summary of the research conducted on reinforcement fine-tuning (RFT) for multimodal large language models (MLLMs). This summary aims to provide readers with a convenient reference and resource for further research.

## Vision (Image)ğŸ‘€ 

### Papers ğŸ“„

* [2505] [OpenThinkIMG] [OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning](https://arxiv.org/abs/2505.08617) [[Model ğŸ¤—](https://huggingface.co/Warrieryes/OpenThinkIMG-Chart-Qwen2-2B-VL)]  [[Datasets ğŸ¤—](https://huggingface.co/collections/Warrieryes/openthinkimg-68244a63e97a24d9b7ffcde9)] [[Code ğŸ’»](https://github.com/zhaochen0110/OpenThinkIMG)]

* [2505] [DanceGRPO (Gen)] [DanceGRPO: Unleashing GRPO on Visual Generation](https://arxiv.org/abs/2505.07818) [[Project ğŸŒ](https://dancegrpo.github.io/)] 
 [[Code ğŸ’»](https://github.com/XueZeyue/DanceGRPO)]

* [2505] [Flow-GRPO (Gen)] [Flow-GRPO: Training Flow Matching Models via Online RL](https://www.arxiv.org/abs/2505.05470) [[Models ğŸ¤—](https://huggingface.co/jieliu)]  [[Code ğŸ’»](https://github.com/yifan123/flow_grpo)]

* [2505] [X-Reasoner] [X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains](https://arxiv.org/abs/2505.03981) [[Code ğŸ’»](https://github.com/microsoft/x-reasoner)]

* [2505] [T2I-R1 (Gen)] [T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT](https://arxiv.org/abs/2505.00703)  [[Code ğŸ’»](https://github.com/CaraJ7/T2I-R1)]

* [2504] [FAST] [Fast-Slow Thinking for Large Vision-Language Model Reasoning](https://arxiv.org/abs/2504.18458) [[Code ğŸ’»](https://github.com/Mr-Loevan/FAST)]

* [2504] [Skywork R1V2] [Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning](https://arxiv.org/abs/2504.16656) [[Models ğŸ¤—](https://huggingface.co/collections/Skywork/skywork-r1v2-68075a3d947a5ae160272671)]  [[Code ğŸ’»](https://github.com/SkyworkAI/Skywork-R1V)]

* [2504] [Relation-R1] [Relation-R1: Cognitive Chain-of-Thought Guided Reinforcement Learning for Unified Relational Comprehension](https://arxiv.org/abs/2504.14642) [[Code ğŸ’»](https://github.com/HKUST-LongGroup/Relation-R1)]

* [2504] [R1-SGG] [Compile Scene Graphs with Reinforcement Learning](https://www.arxiv.org/abs/2504.13617) [[Code ğŸ’»](https://github.com/gpt4vision/R1-SGG)]

* [2504] [NoisyRollout] [Reinforcing Visual Reasoning with Data Augmentation](https://arxiv.org/abs/2504.13055) [[Models ğŸ¤—](https://huggingface.co/collections/xyliu6/noisyrollout-67ff992d1cf251087fe021a2)]  [[Datasets ğŸ¤—](https://huggingface.co/collections/xyliu6/noisyrollout-67ff992d1cf251087fe021a2)] [[Code ğŸ’»](https://github.com/John-AI-Lab/NoisyRollout)]

* [2504] [SimpleAR (Gen)] [SimpleAR: Pushing the Frontier of Autoregressive Visual Generation through Pretraining, SFT, and RL](https://arxiv.org/abs/2504.11455) [[Models ğŸ¤—](https://huggingface.co/collections/Daniel0724/simplear-6805053f5b4b9961ac025136)]  [[Code ğŸ’»](https://github.com/wdrink/SimpleAR)]

* [2504] [VL-Rethinker] [Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning](https://arxiv.org/abs/2504.08837) [[Project ğŸŒ](https://tiger-ai-lab.github.io/VL-Rethinker/)] [[Models ğŸ¤—](https://huggingface.co/collections/TIGER-Lab/vl-rethinker-67fdc54de07c90e9c6c69d09)] [[Dataset ğŸ¤—](https://huggingface.co/datasets/TIGER-Lab/ViRL39K)] [[Code ğŸ’»](https://github.com/TIGER-AI-Lab/VL-Rethinker)]

* [2504] [Kimi-VL] [Kimi-VL Technical Report](https://arxiv.org/abs/2504.07491) [[Project ğŸŒ](https://github.com/MoonshotAI/Kimi-VL)] [[Models ğŸ¤—](https://huggingface.co/collections/moonshotai/kimi-vl-a3b-67f67b6ac91d3b03d382dd85)] [[Demo ğŸ¤—](https://huggingface.co/spaces/moonshotai/Kimi-VL-A3B-Thinking)] [[Code ğŸ’»](https://github.com/MoonshotAI/Kimi-VL)]

* [2504] [VLAA-Thinking] [SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models](https://github.com/UCSC-VLAA/VLAA-Thinking/blob/main/assets/VLAA-Thinker.pdf)  [[Models ğŸ¤—](https://huggingface.co/collections/UCSC-VLAA/vlaa-thinker-67eda033419273423d77249e)]  [[Dataset ğŸ¤—](https://huggingface.co/datasets/UCSC-VLAA/VLAA-Thinking)]  [[Code ğŸ’»](https://github.com/UCSC-VLAA/VLAA-Thinking)]

* [2504] [Perception-R1] [Perception-R1: Pioneering Perception Policy with Reinforcement Learning](https://arxiv.org/abs/2504.07954) [[Models ğŸ¤—](https://huggingface.co/collections/Kangheng/perception-r1-67f6b14f89d307a0ece985af)]  [[Datasets ğŸ¤—](https://huggingface.co/collections/Kangheng/perception-r1-67f6b14f89d307a0ece985af)]  [[Code ğŸ’»](https://github.com/linkangheng/PR1)]

* [2504] [SoTA with Less] [SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement](https://arxiv.org/abs/2504.07934) [[Model ğŸ¤—](https://huggingface.co/russwang/ThinkLite-VL-7B)]  [[Datasets ğŸ¤—](https://huggingface.co/collections/russwang/thinklite-vl-67f88c6493f8a7601e73fe5a)]  [[Code ğŸ’»](https://github.com/si0wang/ThinkLite-VL)]

* [2504] [VLM-R1] [VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model](https://arxiv.org/abs/2504.07615) [[Model ğŸ¤—](https://huggingface.co/omlab/Qwen2.5VL-3B-VLM-R1-REC-500steps)]  [[Dataset ğŸ¤—](https://huggingface.co/datasets/omlab/VLM-R1)] [[Demo ğŸ¤—](https://huggingface.co/spaces/omlab/VLM-R1-Referral-Expression)] [[Code ğŸ’»](https://github.com/om-ai-lab/VLM-R1)]

* [2504] [CrowdVLM-R1] [CrowdVLM-R1: Expanding R1 Ability to Vision Language Model for Crowd Counting using Fuzzy Group Relative Policy Reward](https://arxiv.org/abs/2504.03724) [[Dataset ğŸ¤—](https://huggingface.co/datasets/yeyimilk/CrowdVLM-R1-data)] [[Code ğŸ’»](https://github.com/yeyimilk/CrowdVLM-R1)]

* [2504] [MAYE] [Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme](https://www.arxiv.org/abs/2504.02587) [[Dataset ğŸ¤—](https://huggingface.co/datasets/ManTle/MAYE)] [[Code ğŸ’»](https://github.com/GAIR-NLP/MAYE)]

* [2503] [Q-Insight] [Q-Insight: Understanding Image Quality via Visual Reinforcement Learning](https://arxiv.org/abs/2503.22679) [[Code ğŸ’»](https://github.com/lwq20020127/Q-Insight)]

* [2503] [Reason-RFT] [Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning](https://arxiv.org/abs/2503.20752) [[Project ğŸŒ](https://tanhuajie.github.io/ReasonRFT)] [[Dataset ğŸ¤—](https://huggingface.co/datasets/tanhuajie2001/Reason-RFT-CoT-Dataset)] [[Code ğŸ’»](https://github.com/tanhuajie/Reason-RFT)]

* [2503] [OpenVLThinker] [OpenVLThinker: An Early Exploration to Vision-Language Reasoning via Iterative Self-Improvement](https://arxiv.org/abs/2503.17352)  [[Model ğŸ¤—](https://huggingface.co/ydeng9/OpenVLThinker-7B)] [[Code ğŸ’»](https://github.com/yihedeng9/OpenVLThinker)]

* [2503] [Think or Not Think] [Think or Not Think: A Study of Explicit Thinking in Rule-Based Visual Reinforcement Fine-Tuning](https://arxiv.org/abs/2503.16188) [[Models ğŸ¤—](https://huggingface.co/afdsafas)] [[Datasets ğŸ¤—](https://huggingface.co/afdsafas)] [[Code ğŸ’»](https://github.com/minglllli/CLS-RL)]

* [2503] [OThink-MR1] [OThink-MR1: Stimulating multimodal generalized reasoning capabilities via dynamic reinforcement learning](https://arxiv.org/abs/2503.16081) 

* [2503] [R1-VL] [R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization](https://arxiv.org/abs/2503.12937) [[Model ğŸ¤—](https://huggingface.co/jingyiZ00)] [[Code ğŸ’»](https://github.com/jingyi0000/R1-VL)]

* [2503] [Skywork R1V] [Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought](https://github.com/SkyworkAI/Skywork-R1V/blob/main/Skywork_R1V.pdf) [[Model ğŸ¤—](https://huggingface.co/Skywork/Skywork-R1V-38B)] [[Code ğŸ’»](https://github.com/SkyworkAI/Skywork-R1V)]

* [2503] [R1-Onevision] [R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization](https://arxiv.org/abs/2503.10615) [[Model ğŸ¤—](https://huggingface.co/Fancy-MLLM/R1-Onevision-7B)]  [[Dataset ğŸ¤—](https://huggingface.co/datasets/Fancy-MLLM/R1-Onevision)] [[Demo ğŸ¤—](https://huggingface.co/spaces/Fancy-MLLM/R1-Onevision)] [[Code ğŸ’»](https://github.com/Fancy-MLLM/R1-Onevision)]

* [2503] [VisualPRM] [VisualPRM: An Effective Process Reward Model for Multimodal Reasoning](https://arxiv.org/abs/2503.10291v1)  [[Project ğŸŒ](https://internvl.github.io/blog/2025-03-13-VisualPRM/)]  [[Model ğŸ¤—](https://huggingface.co/OpenGVLab/VisualPRM-8B)]  [[Dataset ğŸ¤—](https://huggingface.co/datasets/OpenGVLab/VisualPRM400K)] [[Benchmark ğŸ¤—](https://huggingface.co/datasets/OpenGVLab/VisualProcessBench)] 

* [2503] [LMM-R1] [LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL](https://arxiv.org/abs/2503.07536)  [[Code ğŸ’»](https://github.com/TideDra/lmm-r1)]

* [2503] [Curr-ReFT] [Boosting the Generalization and Reasoning of Vision Language Models with Curriculum Reinforcement Learning](https://arxiv.org/abs/2503.07065) [[Models ğŸ¤—](https://huggingface.co/ZTE-AIM)] [[Dataset ğŸ¤—](https://huggingface.co/datasets/ZTE-AIM/Curr-ReFT-data)] [[Code ğŸ’»](https://github.com/ding523/Curr_REFT)]

* [2503] [VisualThinker-R1-Zero] [R1-Zero's "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model](https://arxiv.org/abs/2503.05132) [[Code ğŸ’»](https://github.com/turningpoint-ai/VisualThinker-R1-Zero)]

* [2503] [Vision-R1] [Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models](https://arxiv.org/abs/2503.06749) [[Code ğŸ’»](https://github.com/Osilly/Vision-R1)]

* [2503] [Seg-Zero] [Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive Reinforcement](https://arxiv.org/abs/2503.06520) [[Model ğŸ¤—](https://huggingface.co/Ricky06662/Seg-Zero-7B)] [[Dataset ğŸ¤—](https://huggingface.co/datasets/Ricky06662/refCOCOg_2k_840)] [[Code ğŸ’»](https://github.com/dvlab-research/Seg-Zero)]

* [2503] [MM-Eureka] [MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning](https://github.com/ModalMinds/MM-EUREKA/blob/main/MM_Eureka_paper.pdf) [[Models ğŸ¤—](https://huggingface.co/FanqingM)] [[Dataset ğŸ¤—](https://huggingface.co/datasets/FanqingM/MM-Eureka-Dataset)] [[Code ğŸ’»](https://github.com/ModalMinds/MM-EUREKA)]

* [2503] [Visual-RFT] [Visual-RFT: Visual Reinforcement Fine-Tuning](https://arxiv.org/abs/2503.01785) [[Project ğŸŒ](https://github.com/Liuziyu77/Visual-RFT)] [[Datasets ğŸ¤—](https://huggingface.co/collections/laolao77/virft-datasets-67bc271b6f2833eccc0651df)] [[Code ğŸ’»](https://github.com/Liuziyu77/Visual-RFT)]

* [2501] [PARM++ (Gen)] [Can We Generate Images with CoT? Letâ€™s Verify and Reinforce Image Generation Step by Step](https://arxiv.org/abs/2501.13926)  [[Project ğŸŒ](https://github.com/ZiyuGuo99/Image-Generation-CoT)] [[Model ğŸ¤—](https://huggingface.co/ZiyuG/Image-Generation-CoT)]  [[Code ğŸ’»](https://github.com/ZiyuGuo99/Image-Generation-CoT)]

* [2501] [Kimi k1.5] [Kimi k1.5: Scaling Reinforcement Learning with LLMs](https://arxiv.org/abs/2501.12599) [[Project ğŸŒ](https://github.com/MoonshotAI/Kimi-k1.5)]
  
* [2501] [Virgo] [Virgo: A Preliminary Exploration on Reproducing o1-like MLLM](https://arxiv.org/abs/2501.01904v2) [[Model ğŸ¤—](https://huggingface.co/RUC-AIBOX/Virgo-72B)] [[Code ğŸ’»](https://github.com/RUCAIBox/Virgo)]

* [2412] [Mulberry] [Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search](https://arxiv.org/abs/2412.18319) [[Model ğŸ¤—](https://huggingface.co/HuanjinYao/Mulberry_llava_8b)] [[Code ğŸ’»](https://github.com/HJYao00/Mulberry)]

* [2411] [Insight-V] [Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models](https://arxiv.org/abs/2411.14432) [[Model ğŸ¤—](https://huggingface.co/collections/THUdyh/insight-v-673f5e1dd8ab5f2d8d332035)] [[Code ğŸ’»](https://github.com/dongyh20/Insight-V)]

* [2411] [InternVL2-MPO] [Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization](https://arxiv.org/abs/2411.10442)  [[Project ğŸŒ](https://internvl.github.io/blog/2024-11-14-InternVL-2.0-MPO/)]  [[Model ğŸ¤—](https://huggingface.co/OpenGVLab/InternVL2-8B-MPO)] [[Code ğŸ’»](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/internvl2.0_mpo)]

### Open-Source Projects (Repository without Paper)ğŸŒ

* [[R1-V ğŸ’»](https://github.com/Deep-Agent/R1-V)]  ![R1-V](https://img.shields.io/github/stars/Deep-Agent/R1-V) [[Code ğŸ’»](https://github.com/Deep-Agent/R1-V)]  [[Datasets ğŸ¤—](https://huggingface.co/collections/MMInstruction/r1-v-67aae24fa56af9d2e2755f82)]  [[Blog ğŸ¯](https://deepagent.notion.site/rlvr-in-vlms)] 

* [[Multimodal Open R1 ğŸ’»](https://github.com/EvolvingLMMs-Lab/open-r1-multimodal)]  ![Multimodal Open R1](https://img.shields.io/github/stars/EvolvingLMMs-Lab/open-r1-multimodal) [[Code ğŸ’»](https://github.com/EvolvingLMMs-Lab/open-r1-multimodal)] [[Model ğŸ¤—](https://huggingface.co/lmms-lab/Qwen2-VL-2B-GRPO-8k)] [[Dataset ğŸ¤—](https://huggingface.co/datasets/lmms-lab/multimodal-open-r1-8k-verified)]

* [[MMR1 ğŸ’»](https://github.com/LengSicong/MMR1)] ![LengSicong/MMR1](https://img.shields.io/github/stars/LengSicong/MMR1) [[Code ğŸ’»](https://github.com/LengSicong/MMR1)] [[Model ğŸ¤—](https://huggingface.co/MMR1/MMR1-Math-v0-7B)] [[Dataset ğŸ¤—](https://huggingface.co/datasets/MMR1/MMR1-Math-RL-Data-v0)] 

* [[R1-Multimodal-Journey ğŸ’»](https://github.com/FanqingM/R1-Multimodal-Journey)] ![R1-Multimodal-Journey](https://img.shields.io/github/stars/FanqingM/R1-Multimodal-Journey) [[Code ğŸ’»](https://github.com/FanqingM/R1-Multimodal-Journey)] (Latest progress at [[MM-Eureka ğŸ’»](https://github.com/ModalMinds/MM-EUREKA)])

* [[R1-Vision ğŸ’»](https://github.com/yuyq96/R1-Vision)] ![R1-Vision](https://img.shields.io/github/stars/yuyq96/R1-Vision) [[Code ğŸ’»](https://github.com/yuyq96/R1-Vision)] [[Cold-Start Datasets ğŸ¤—](https://huggingface.co/collections/yuyq96/r1-vision-67a6fb7898423dca453efa83)]

* [[Ocean-R1 ğŸ’»](https://github.com/VLM-RL/Ocean-R1)]  ![Ocean-R1](https://img.shields.io/github/stars/VLM-RL/Ocean-R1) [[Code ğŸ’»](https://github.com/VLM-RL/Ocean-R1)] [[Models ğŸ¤—](https://huggingface.co/minglingfeng)] [[Datasets ğŸ¤—](https://huggingface.co/minglingfeng)]

* [[R1V-Free ğŸ’»](https://github.com/Exgc/R1V-Free)]  ![Exgc/R1V-Free](https://img.shields.io/github/stars/Exgc/R1V-Free) [[Code ğŸ’»](https://github.com/Exgc/R1V-Free)] [[Models ğŸ¤—](https://huggingface.co/collections/Exgc/r1v-free-67f769feedffab8761b8f053)] [[Dataset ğŸ¤—](https://huggingface.co/datasets/Exgc/R1V-Free_RLHFV)]

* [[SeekWorld ğŸ’»](https://github.com/TheEighthDay/SeekWorld)]  ![TheEighthDay/SeekWorld](https://img.shields.io/github/stars/TheEighthDay/SeekWorld) [[Code ğŸ’»](https://github.com/TheEighthDay/SeekWorld)]  [[Model ğŸ¤—](https://huggingface.co/TheEighthDay/SeekWorld_RL_PLUS)] [[Dataset ğŸ¤—](https://huggingface.co/datasets/TheEighthDay/SeekWorld)] [[Demo ğŸ¤—](https://huggingface.co/spaces/TheEighthDay/SeekWorld_APP)]

* [[R1-Track ğŸ’»](https://github.com/Wangbiao2/R1-Track)]  ![Wangbiao2/R1-Track](https://img.shields.io/github/stars/Wangbiao2/R1-Track) [[Code ğŸ’»](https://github.com/Wangbiao2/R1-Track)] [[Models ğŸ¤—](https://huggingface.co/WangBiao)] [[Datasets ğŸ¤—](https://huggingface.co/WangBiao)]

## Vision (Video)ğŸ“¹ 

### Papers ğŸ“„

* [2504] [TinyLLaVA-Video-R1] [TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning](https://arxiv.org/abs/2504.09641) [[Model ğŸ¤—](https://huggingface.co/Zhang199/TinyLLaVA-Video-R1)] [[Code ğŸ’»](https://github.com/ZhangXJ199/TinyLLaVA-Video-R1)]

* [2504] [VideoChat-R1] [VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning](https://arxiv.org/abs/2504.06958)  [[Model ğŸ¤—](https://huggingface.co/collections/OpenGVLab/videochat-r1-67fbe26e4eb08c83aa24643e)] [[Code ğŸ’»](https://github.com/OpenGVLab/VideoChat-R1)]

* [2504] [Spatial-R1] [Spatial-R1: Enhancing MLLMs in Video Spatial Reasoning](https://arxiv.org/abs/2504.01805) [[Model ğŸ¤—](https://huggingface.co/RUBBISHLIKE/SpaceR)] [[Datasets ğŸ¤—](https://huggingface.co/RUBBISHLIKE)] [[Code ğŸ’»](https://github.com/OuyangKun10/SpaceR)]

* [2504] [R1-Zero-VSI] [Improved Visual-Spatial Reasoning via R1-Zero-Like Training](https://arxiv.org/abs/2504.00883) [[Code ğŸ’»](https://github.com/zhijie-group/R1-Zero-VSI)]

* [2503] [SEED-Bench-R1] [Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1](https://arxiv.org/abs/2503.24376) [[Dataset ğŸ¤—](https://huggingface.co/datasets/TencentARC/SEED-Bench-R1)]  [[Code ğŸ’»](https://github.com/TencentARC/SEED-Bench-R1)]

* [2503] [Video-R1] [Video-R1: Reinforcing Video Reasoning in MLLMs](https://arxiv.org/abs/2503.21776) [[Model ğŸ¤—](https://huggingface.co/Video-R1/Video-R1-7B)] [[Dataset ğŸ¤—](https://huggingface.co/datasets/Video-R1/Video-R1-data)] [[Code ğŸ’»](https://github.com/tulerfeng/Video-R1)]

* [2503] [TimeZero] [TimeZero: Temporal Video Grounding with Reasoning-Guided LVLM](https://arxiv.org/abs/2503.13377) [[Model ğŸ¤—](https://huggingface.co/wwwyyy/TimeZero-Charades-7B)] [[Code ğŸ’»](https://github.com/www-Ye/TimeZero)]

### Open-Source Projects (Repository without Paper)ğŸŒ

* [[Open R1 Video ğŸ’»](https://github.com/Wang-Xiaodong1899/Open-R1-Video)] ![Open R1 Video](https://img.shields.io/github/stars/Wang-Xiaodong1899/Open-R1-Video) [[Code ğŸ’»](https://github.com/Wang-Xiaodong1899/Open-R1-Video)] [[Model ğŸ¤—](https://huggingface.co/Xiaodong/Open-R1-Video-7B)]  [[Dataset ğŸ¤—](https://huggingface.co/datasets/Xiaodong/open-r1-video-4k)]

* [[Temporal-R1 ğŸ’»](https://github.com/appletea233/Temporal-R1)]  ![Temporal-R1](https://img.shields.io/github/stars/appletea233/Temporal-R1) [[Code ğŸ’»](https://github.com/appletea233/Temporal-R1)] [[Models ğŸ¤—](https://huggingface.co/appletea2333)]

* [[Open-LLaVA-Video-R1 ğŸ’»](https://github.com/Hui-design/Open-LLaVA-Video-R1)] ![Open-LLaVA-Video-R1](https://img.shields.io/github/stars/Hui-design/Open-LLaVA-Video-R1) [[Code ğŸ’»](https://github.com/Hui-design/Open-LLaVA-Video-R1)]

## Medical VisionğŸ¥ 

### Papers ğŸ“„

* [2504] [ChestX-Reasoner] [ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification](https://arxiv.org/pdf/2504.20930) 

* [2503] [Med-R1] [Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in Vision-Language Models](https://arxiv.org/abs/2503.13939v3) [[Model ğŸ¤—](https://huggingface.co/yuxianglai117/Med-R1)] [[Code ğŸ’»](https://github.com/Yuxiang-Lai117/Med-R1)]

* [2502] [MedVLM-R1] [MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning](https://arxiv.org/abs/2502.19634) [[Model ğŸ¤—](https://huggingface.co/JZPeterPan/MedVLM-R1)]

## Embodied VisionğŸ¤– 

### Papers ğŸ“„

* [2504] [Embodied-R] [Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning](https://arxiv.org/abs/2504.12680) [[Code ğŸ’»](https://github.com/EmbodiedCity/Embodied-R.code)]

* [2503] [Embodied-Reasoner] [Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks](https://arxiv.org/abs/2503.21696v1) [[Project ğŸŒ](https://embodied-reasoner.github.io/)] [[Dataset ğŸ¤—](https://huggingface.co/datasets/zwq2018/embodied_reasoner)] [[Code ğŸ’»](https://github.com/zwq2018/embodied_reasoner)]

## Multimodal Reward Model ğŸ’¯

### Papers ğŸ“„

* [2505] [Skywork-VL Reward] [Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning](https://arxiv.org/abs/2505.07263) [[Model ğŸ¤—](https://huggingface.co/Skywork/Skywork-VL-Reward-7B)] [[Code ğŸ’»](https://github.com/SkyworkAI/Skywork-R1V)]

* [2505] [UnifiedReward-Think] [Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.03318)  [[Project ğŸŒ](https://codegoat24.github.io/UnifiedReward/think)] [[Models ğŸ¤—](https://huggingface.co/collections/CodeGoat24/unifiedreward-models-67c3008148c3a380d15ac63a)] [[Datasets ğŸ¤—](https://huggingface.co/collections/CodeGoat24/unifiedreward-training-data-67c300d4fd5eff00fa7f1ede)] [[Code ğŸ’»](https://github.com/CodeGoat24/UnifiedReward)]

* [2505] [R1-Reward] [R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning](https://arxiv.org/abs/2505.02835) [[Model ğŸ¤—](https://huggingface.co/yifanzhang114/R1-Reward)]  [[Dataset ğŸ¤—](https://huggingface.co/datasets/yifanzhang114/R1-Reward-RL)] [[Code ğŸ’»](https://github.com/yfzhang114/r1_reward)]

## AudioğŸ‘‚

### Papers ğŸ“„

* [2504] [SARI] [SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning](https://arxiv.org/pdf/2504.15900)

* [2503] [R1-AQA] [Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study on Audio Question Answering](https://arxiv.org/abs/2503.11197v2)  [[Model ğŸ¤—](https://huggingface.co/mispeech/r1-aqa)] [[Code ğŸ’»](https://github.com/xiaomi-research/r1-aqa)]

* [2503] [Audio-Reasoner] [Audio-Reasoner: Improving Reasoning Capability in Large Audio Language Models](https://arxiv.org/abs/2503.02318) [[Project ğŸŒ](https://xzf-thu.github.io/Audio-Reasoner/)] [[Model ğŸ¤—](https://huggingface.co/zhifeixie/Audio-Reasoner)] [[Code ğŸ’»](https://github.com/xzf-thu/Audio-Reasoner)]

## Omniâ˜ºï¸

### Papers ğŸ“„

* [2505] [EchoInk-R1] [EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning](https://arxiv.org/pdf/2505.04623) [[Model ğŸ¤—](https://huggingface.co/harryhsing/EchoInk-R1-7B)] [[Dataset ğŸ¤—](https://huggingface.co/datasets/harryhsing/OmniInstruct_V1_AVQA_R1)] [[Code ğŸ’»](https://github.com/HarryHsing/EchoInk)]

* [2503] [R1-Omni] [R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcement Learning](https://arxiv.org/abs/2503.05379)  [[Model ğŸ¤—](https://huggingface.co/StarJiaxing/R1-Omni-0.5B)] [[Code ğŸ’»](https://github.com/HumanMLLM/R1-Omni)]

## GUIğŸ“²

### Papers ğŸ“„

* [2504] [InfiGUI-R1] [InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners](https://arxiv.org/pdf/2504.14239) [[Model ğŸ¤—](https://huggingface.co/Reallm-Labs/InfiGUI-R1-3B)] [[Code ğŸ’»](https://github.com/Reallm-Labs/InfiGUI-R1)]

* [2504] [GUI-R1] [GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents](https://arxiv.org/abs/2504.10458) [[Model ğŸ¤—](https://huggingface.co/ritzzai/GUI-R1)] [[Dataset ğŸ¤—](https://huggingface.co/datasets/ritzzai/GUI-R1)] [[Code ğŸ’»](https://github.com/ritzz-ai/GUI-R1)]

* [2503] [UI-R1] [UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning](https://arxiv.org/abs/2503.21620) 

## Framework ğŸ—¼

### Open-Source Project (Repository without Paper)ğŸŒ

* [[EasyR1 ğŸ’»](https://github.com/hiyouga/EasyR1)]  ![EasyR1](https://img.shields.io/github/stars/hiyouga/EasyR1) [[Code ğŸ’»](https://github.com/hiyouga/EasyR1)] (An Efficient, Scalable, Multi-Modality RL Training Framework)

## MetaverseğŸŒ 

### Paper ğŸ“„

* [2503] [MetaSpatial] [MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse](https://arxiv.org/abs/2503.18470) [[Dataset ğŸ¤—](https://huggingface.co/datasets/zhenyupan/3d_layout_reasoning)] [[Code ğŸ’»](https://github.com/PzySeere/MetaSpatial)]

## Agents ğŸ‘¥

### Open-Source Project (Repository without Paper)ğŸŒ

* [[VAGEN ğŸ’»](https://github.com/RAGEN-AI/VAGEN)] ![VAGEN](https://img.shields.io/github/stars/RAGEN-AI/VAGEN) [[Code ğŸ’»](https://github.com/RAGEN-AI/VAGEN)]

